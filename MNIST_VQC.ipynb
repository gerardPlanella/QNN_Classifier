{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPMSZUU0ZHRYYHFr1IVIe7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerardPlanella/QNN_Classifier/blob/main/MNIST_VQC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Variational Quantum Classifier"
      ],
      "metadata": {
        "id": "77zx2kUZjIU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Installation"
      ],
      "metadata": {
        "id": "Hb58wDPijPVv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9Juiw8BjHj-"
      },
      "outputs": [],
      "source": [
        "!pip install -q pennylane "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "YQqunCptjeot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "hLQIQrITml67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_pre = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
        "                                                transforms.Compose([transforms.ToTensor()]))\n",
        "test_set_pre = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
        "                                               transforms.Compose([transforms.ToTensor()]))  "
      ],
      "metadata": {
        "id": "5d1Q0Xt9js4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Definition"
      ],
      "metadata": {
        "id": "HCNAJFfzmpTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_qubits = 6\n",
        "num_layers = 8\n",
        "batch_size = 100\n",
        "\n",
        "reduced_dataset = True\n",
        "reduced_classes = [1,2,3,7]   \n",
        "reduced_num_classes = len(reduced_classes)\n"
      ],
      "metadata": {
        "id": "Xxjy-ptAmwR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Reduction"
      ],
      "metadata": {
        "id": "Tb7fu8XLo5qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if reduced_dataset:\n",
        "    train_filtered_indices = [i for i in range(len(train_set_pre)) if train_set_pre.targets[i] in reduced_classes]\n",
        "    train_filtered_pre = Subset(train_set_pre, train_filtered_indices)\n",
        "    train_set_pre.targets[train_filtered_indices] = torch.tensor([reduced_classes.index(train_set_pre.targets[i].item()) for i in train_filtered_indices])\n",
        "\n",
        "    test_filtered_indices = [i for i in range(len(test_set_pre)) if test_set_pre.targets[i] in reduced_classes]\n",
        "    test_filtered_pre = Subset(test_set_pre, test_filtered_indices)\n",
        "    test_set_pre.targets[test_filtered_indices] = torch.tensor([reduced_classes.index(test_set_pre.targets[i].item()) for i in test_filtered_indices])\n",
        "\n",
        "    train_loader_pre = torch.utils.data.DataLoader(train_filtered_pre, batch_size=100, shuffle = True)\n",
        "    test_loader_pre = torch.utils.data.DataLoader(test_filtered_pre, batch_size=100, shuffle = True)\n",
        "else:\n",
        "  train_loader_pre = torch.utils.data.DataLoader(train_set_pre, \n",
        "                                           batch_size=100)\n",
        "  test_loader_pre = torch.utils.data.DataLoader(test_set_pre,\n",
        "                                            batch_size=100)"
      ],
      "metadata": {
        "id": "vxf2fBBUnsPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for val in train_loader_pre:\n",
        "  label = val[1][0]\n",
        "  img = val[0][0]\n",
        "  print(\"Label: \" + str(label))\n",
        "  print(\"Image Size: \" + str(img.size()))\n",
        "  plt.imshow(img.squeeze(), cmap='gray')\n",
        "  input_dim = img.size()[1]**2\n",
        "  break"
      ],
      "metadata": {
        "id": "GQ9dN06HowVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder"
      ],
      "metadata": {
        "id": "1ZKN0DcUrl_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 2 ** n_qubits\n",
        "\n",
        "\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        input_dim = kwargs[\"input_dim\"]\n",
        "        encoder_dim = kwargs[\"encoder_dims\"]\n",
        "        latent_dim = kwargs[\"latent_dim\"]\n",
        "        self.encoder = nn.Sequential(OrderedDict([ ('fc1', nn.Linear(input_dim, encoder_dim[0])),\n",
        "                                    ('act1', nn.ReLU()),\n",
        "                                    ('fc2', nn.Linear(encoder_dim[0], encoder_dim[1])),\n",
        "                                    ('act2', nn.ReLU()),\n",
        "                                    ('fc3', nn.Linear(encoder_dim[1], latent_dim)) ,\n",
        "                                    ('act3', nn.Sigmoid())\n",
        "                                ]))\n",
        "        \n",
        "        self.decoder = nn.Sequential(OrderedDict([ ('fc1', nn.Linear(latent_dim, encoder_dim[1])),\n",
        "                                    ('act1', nn.ReLU()),\n",
        "                                    ('fc2', nn.Linear(encoder_dim[1], encoder_dim[0])),\n",
        "                                    ('act2', nn.ReLU()),\n",
        "                                    ('fc3', nn.Linear(encoder_dim[0], input_dim)),\n",
        "                                    ('act3', nn.ReLU())\n",
        "                                ]))\n",
        "\n",
        "    def forward(self, features):\n",
        "        encoded = self.encoder(features)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "6GoMSojerpGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoder Training"
      ],
      "metadata": {
        "id": "A2g5uIM-uo2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AE(input_dim=input_dim, latent_dim = latent_dim, encoder_dims = [196, 64]).to(device)\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "kiT6tPbmuZup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    loss = 0\n",
        "    for batch_features, _ in train_loader_pre:\n",
        "        batch_features = batch_features.view(-1, input_dim).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_features)\n",
        "        train_loss = criterion(outputs, batch_features)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        loss += train_loss.item()\n",
        "    loss = loss / len(train_loader_pre)\n",
        "    \n",
        "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
      ],
      "metadata": {
        "id": "RwehLoc-vRuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Encoding"
      ],
      "metadata": {
        "id": "Cz2JyMBfzPlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderOutputDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encoder_outputs, labels):\n",
        "        self.encoder_outputs = encoder_outputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoder_outputs[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoder_outputs)"
      ],
      "metadata": {
        "id": "uQBXUp3c0Mya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "encoder_outputs = []\n",
        "labels = []\n",
        "with torch.no_grad():\n",
        "  for batch_features, batch_labels in train_loader_pre:\n",
        "      batch_features = batch_features.view(-1, input_dim).to(device)\n",
        "      encoded_features = model.encoder(batch_features)\n",
        "      encoder_outputs.append(encoded_features.squeeze())  # Store the encoder output\n",
        "      labels.append(label)\n",
        "    \n",
        "  train_data = torch.cat(encoder_outputs, 0 )\n",
        "  train_labels = torch.cat(labels, 0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_features, batch_labels in test_loader_pre:\n",
        "      batch_features = batch_features.view(-1, input_dim).to(device)\n",
        "      encoded_features = model.encoder(batch_features)\n",
        "      encoder_outputs.append(encoded_features.squeeze())  # Store the encoder output\n",
        "      labels.append(label)\n",
        "    \n",
        "  test_data = torch.cat(encoder_outputs, 0 )\n",
        "  test_labels = torch.cat(labels, 0)\n",
        "\n",
        "\n",
        "train_set = EncoderOutputDataset(train_data, train_labels)\n",
        "test_set = EncoderOutputDataset(test_data, test_labels)\n",
        "\n",
        "train_set_loader = torch.utils.data.DataLoader(train_set, \n",
        "                                           batch_size=8,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "test_set_loader = torch.utils.data.DataLoader(test_set, \n",
        "                                           batch_size=8,\n",
        "                                           shuffle = True)"
      ],
      "metadata": {
        "id": "sfwpTMEGzUSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variational Quantum Classifier with PennyLane"
      ],
      "metadata": {
        "id": "K4_oONSu5bxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as p_np\n",
        "\n",
        "from pennylane.templates.state_preparations import MottonenStatePreparation\n",
        "from pennylane.templates.layers import StronglyEntanglingLayers"
      ],
      "metadata": {
        "id": "FcRvCcjY5gv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Circuit Definition"
      ],
      "metadata": {
        "id": "23z2uYZ27cfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev = qml.device(\"default.qubit\", wires = n_qubits)\n",
        "weight_shapes = {\"weights\": (num_layers, n_qubits, 3)}\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def circuit(inputs, weights):\n",
        "  num_layers = weights.shape[0]\n",
        "  n_qubits = weights.shape[1]\n",
        "  weights_each_layer = torch.split(weights, num_layers, dim=0)\n",
        "  # Input normalization\n",
        "  inputs_1 = inputs / torch.sqrt(torch.sum(inputs ** 2, dim=-1).clamp(min=0.001))\n",
        "\n",
        "  for i, W in enumerate(weights):\n",
        "    # Data re-uploading technique\n",
        "    if i % 2 == 0:\n",
        "      MottonenStatePreparation(inputs_1, wires = range(n_qubits))\n",
        "    \n",
        "  # Neural network layer\n",
        "  StronglyEntanglingLayers(weights_each_layer[0], wires=range(n_qubits))\n",
        "  \n",
        "  # Measurement return\n",
        "  return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]"
      ],
      "metadata": {
        "id": "7Flh6VT75rQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.rand(weight_shapes[\"weights\"])\n",
        "print(qml.draw(circuit, expansion_strategy=\"device\")(torch.rand(64), weights))"
      ],
      "metadata": {
        "id": "DIFA8o4cJC_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum Neural Network Model"
      ],
      "metadata": {
        "id": "nVQmoECN7fiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QVC(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.input_dim = kwargs[\"input_dim\"]\n",
        "        self.num_classes = kwargs[\"num_classes\"]\n",
        "        self.weight_shapes = kwargs[\"weight_shapes\"]\n",
        "        \n",
        "        self.model = nn.Sequential(OrderedDict([ \n",
        "            ('quantum_layer', qml.qnn.TorchLayer(kwargs[\"qnode\"], self.weight_shapes)),\n",
        "            ('fc', nn.Linear(self.weight_shapes[\"weights\"][1], self.num_classes))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, input):\n",
        "      return self.model(input)"
      ],
      "metadata": {
        "id": "nBgjQPDv5ylu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qvc = QVC(input_dim = latent_dim, num_classes = len(reduced_classes), weight_shapes = weight_shapes, qnode = circuit).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(qvc.parameters(), lr=1e-3)\n",
        "print(qvc)"
      ],
      "metadata": {
        "id": "3AGBk_-_BFu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "26zgXKkUCA_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "qvc.train()\n",
        "for epoch in range(epochs):\n",
        "    loss = 0\n",
        "    for batch_features, labels in train_set_loader:\n",
        "        batch_features = batch_features.to(device)\n",
        "        labels = labels.to(device)\n",
        "        opt.zero_grad()\n",
        "        outputs = qvc(batch_features)\n",
        "        train_loss = criterion(outputs, labels)\n",
        "        train_loss.backward(retain_graph = True)\n",
        "        opt.step()\n",
        "        loss += train_loss.item()\n",
        "    loss = loss / len(train_set_loader)\n",
        "    \n",
        "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
      ],
      "metadata": {
        "id": "izKoyew4CAkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qvc.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_features, labels in test_set_loader:\n",
        "        batch_features = batch_features.to(device)\n",
        "        outputs = qvc(batch_features)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted_labels = torch.max(probabilities, dim=1)\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted_labels == labels).sum().item()\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "id": "D6PZmwcRPhHs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}